{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anuragpandey2005/LoanTap_claasification/blob/main/LoanTap_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vysK0UasYLC"
      },
      "source": [
        "# LoanTap Logistic Regression Case Study\n",
        "\n",
        "## üéØ Objective\n",
        "The objective of this case study is to build a Logistic Regression model to predict whether a loan applicant will **fully repay the loan or default**.  \n",
        "This will help LoanTap to:  \n",
        "- Approve loans for the right customers  \n",
        "- Avoid giving loans to risky customers  \n",
        "- Reduce NPAs (non-performing assets)  \n",
        "- Balance between **profit (more approvals)** and **risk (less defaults)**  \n",
        "\n",
        "---\n",
        "\n",
        "## üìù Tasks to Do\n",
        "\n",
        "### 1. Problem Definition & Exploratory Data Analysis (EDA)\n",
        "- Import the dataset and check its structure  \n",
        "- Understand column data types and summary statistics  \n",
        "- Check missing values and duplicates  \n",
        "- Perform univariate analysis (distribution of variables)  \n",
        "- Perform bivariate analysis (relationship of features with target `loan_status`)  \n",
        "- Check outliers and variable ranges  \n",
        "\n",
        "### 2. Data Preprocessing\n",
        "- Handle missing values  \n",
        "- Treat outliers if required  \n",
        "- Encode categorical variables  \n",
        "- Create new features (flags like `pub_rec_flag`, `mort_acc_flag`, `bankruptcy_flag`)  \n",
        "- Apply feature scaling (StandardScaler / MinMaxScaler)  \n",
        "\n",
        "### 3. Model Building (Logistic Regression)\n",
        "- Split dataset into Train and Test sets  \n",
        "- Build Logistic Regression model  \n",
        "- Check model coefficients and interpret them  \n",
        "\n",
        "### 4. Model Evaluation\n",
        "- Generate Classification Report (Accuracy, Precision, Recall, F1-score)  \n",
        "- Plot Confusion Matrix  \n",
        "- Plot ROC-AUC Curve  \n",
        "- Plot Precision-Recall Curve  \n",
        "- Discuss Precision vs Recall trade-off  \n",
        "\n",
        "### 5. Business Questions / Trade-offs\n",
        "- How to reduce false positives (approve safe borrowers)?  \n",
        "- How to improve recall (catch real defaulters)?  \n",
        "- How to balance between NPAs and Profitability?  \n",
        "\n",
        "### 6. Actionable Insights & Recommendations\n",
        "- Identify top features impacting loan repayment  \n",
        "- Suggest changes in loan approval policy  \n",
        "- Provide risk vs reward strategy for LoanTap  \n",
        "\n",
        "---\n",
        "\n",
        "## üìå Questionnaire to Answer\n",
        "1. What percentage of customers have fully paid their Loan Amount?  \n",
        "2. Correlation between Loan Amount and Installment features.  \n",
        "3. Majority home ownership type.  \n",
        "4. People with grade 'A' are more likely to pay fully (True/False).  \n",
        "5. Top 2 afforded job titles.  \n",
        "6. Primary metric for bank: ROC AUC / Precision / Recall / F1 Score.  \n",
        "7. How precision-recall gap affects the bank.  \n",
        "8. Features that strongly impact outcome.  \n",
        "9. Will geographical location affect results? (Yes/No)  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r48Qgp5xlK6b"
      },
      "outputs": [],
      "source": [
        "# Importing the database\n",
        "!pip install gdown\n",
        "\n",
        "import gdown\n",
        "\n",
        "# Shared link of your file:\n",
        "url = 'https://drive.google.com/file/d/1ZPYj7CZCfxntE8p2Lze_4QO4MyEOy6_d/view?usp=sharing'\n",
        "\n",
        "# Extract file ID manually (1ZPYj7CZCfxntE8p2Lze_4QO4MyEOy6_d)\n",
        "file_id = '1ZPYj7CZCfxntE8p2Lze_4QO4MyEOy6_d'\n",
        "downloaded = gdown.download(f'https://drive.google.com/uc?export=download&id={file_id}', 'LoanTapData.csv', quiet=False)\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.read_csv('LoanTapData.csv')\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5g4AIXhHwF1K"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"LoanTapData.csv\")\n",
        "\n",
        "# üîπ Check if there are any duplicate rows\n",
        "print(df.duplicated().sum())\n",
        "\n",
        "# üîπ Display duplicate rows (if you want to see them)\n",
        "df[df.duplicated()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWhr8P5EwRTs"
      },
      "source": [
        "**THERE IS NO DUPLICATE COLUMN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXUlxXFfvE7E"
      },
      "outputs": [],
      "source": [
        "# Info of Dataset\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxH08-nj88Sc"
      },
      "source": [
        "#### Checking the null columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDAT_g03vHjB"
      },
      "outputs": [],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IH-6QUzvqak"
      },
      "source": [
        "# üìä Columns with Missing Values in LoanTap Dataset\n",
        "\n",
        "| Column Name           | Missing Values Count |\n",
        "|-----------------------|-----------------------|\n",
        "| emp_title             | 22,927               |\n",
        "| emp_length            | 18,301               |\n",
        "| title                 | 1,756                |\n",
        "| revol_util            | 276                  |\n",
        "| mort_acc              | 37,795               |\n",
        "| pub_rec_bankruptcies  | 535                  |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3bHiIdL1MYx"
      },
      "source": [
        "# Univariate Analysis\n",
        "- First I will Treat the columns With NaN Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DQcqc712fyi"
      },
      "outputs": [],
      "source": [
        "df.hist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ia6vi0J19aW_"
      },
      "source": [
        "#### For `emp_title`\n",
        "- Missing Values : 22,927"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idfrRXkavMIt"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Top 10 most frequent job titles\n",
        "top_titles = df['emp_title'].value_counts().nlargest(10)\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.barplot(x=top_titles.values, y=top_titles.index, palette=\"viridis\")\n",
        "\n",
        "plt.title(\"Top 10 Job Titles Distribution\")\n",
        "plt.xlabel(\"Count\")\n",
        "plt.ylabel(\"Job Title\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dyV8-xMn-gkG"
      },
      "outputs": [],
      "source": [
        "# lets Check The Values stored in the \"emp_title\" column\n",
        "df['emp_title'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwD84bDU27ne"
      },
      "source": [
        "##### The Column **emp_title** Too Much variety so we will drop_it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "meJnJsMj3QG1"
      },
      "outputs": [],
      "source": [
        "df.drop('emp_title',axis=1,inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvNJeAk88P4Q"
      },
      "source": [
        "#### For `emp_length`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XebEOUbf3ouB"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Barplot for emp_length\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.countplot(x=\"emp_length\", data=df, order=df['emp_length'].value_counts().index)\n",
        "\n",
        "plt.title(\"Distribution of Employment Length\")\n",
        "plt.xlabel(\"Employment Length\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.xticks(rotation=45)   # rotate labels for better visibility\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-jxGkIn5Ckx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# 1. Clean the values: remove 'years', 'year', '+', '<'\n",
        "def clean_emp_length(x):\n",
        "    if pd.isnull(x):\n",
        "        return np.nan\n",
        "    if x == '< 1 year':\n",
        "        return 0\n",
        "    if x == '10+ years':\n",
        "        return 10\n",
        "    return int(x.strip().split()[0])   # e.g. '4 years' -> 4\n",
        "\n",
        "df['emp_length'] = df['emp_length'].apply(clean_emp_length)\n",
        "\n",
        "# 2. Check median\n",
        "median_val = df['emp_length'].median()\n",
        "print(\"Median Employment Length:\", median_val)\n",
        "\n",
        "# 3. Fill missing with median\n",
        "df['emp_length'] = df['emp_length'].fillna(median_val)\n",
        "\n",
        "# 4. Verify\n",
        "df['emp_length'].value_counts(dropna=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvQsIJ7G8y2k"
      },
      "source": [
        "#### For `title`\n",
        "- missing values 1,756"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FoaZamnf8NY8"
      },
      "outputs": [],
      "source": [
        "# check top 10 most common titles\n",
        "top_titles = df['title'].value_counts().head(10)\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(x=top_titles.values, y=top_titles.index, palette=\"viridis\")\n",
        "plt.title(\"Top 10 Loan Titles\")\n",
        "plt.xlabel(\"Count\")\n",
        "plt.ylabel(\"Loan Title\")\n",
        "plt.show()\n",
        "\n",
        "df['title']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ClgIFpil_gTI"
      },
      "outputs": [],
      "source": [
        "# Making The values lower case and removing the symbols,special characters from the values\n",
        "df['title']=df['title'].str.lower().str.replace(r'[^a-z0-9\\s]', '', regex=True).str.strip()\n",
        "df['title']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5iHID8KAsNF"
      },
      "outputs": [],
      "source": [
        "# Filling the Title NAN Values with Unknown\n",
        "df['title'] = df['title'].fillna(\"Unknown\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZeda_w_BPeY"
      },
      "source": [
        "#### For  `revol_util`\n",
        "- missing values: 276"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQ737_UgBCZH"
      },
      "outputs": [],
      "source": [
        "median_val = df['revol_util'].median()\n",
        "df['revol_util'] = df['revol_util'].fillna(median_val)\n",
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCfv1yhfB1WB"
      },
      "source": [
        "#### for `mort_acc`\n",
        "- missing values: 37795"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUF5_IlBL29R"
      },
      "outputs": [],
      "source": [
        "mortacc_counts = df['mort_acc'].value_counts().sort_index()\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(x=mortacc_counts.index, y=mortacc_counts.values, palette=\"viridis\")\n",
        "plt.title(\"Distribution of Mortgage Accounts\")\n",
        "plt.xlabel(\"Number of Mortgage Accounts\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N68hD_YxH67Z"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,6))\n",
        "sns.histplot(df['mort_acc'], bins=30, kde=False, color=\"skyblue\")\n",
        "plt.title(\"Distribution of Mortgage Accounts\")\n",
        "plt.xlabel(\"Number of Mortgage Accounts\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sU3vOW6HCE4l"
      },
      "outputs": [],
      "source": [
        "# Since,\"mort_acc\" has many missing values we cannot use median,mean directly to treat the missing values\n",
        "# Therefore, Checking the correlation of columns with mort_acc\n",
        "corr_matrix = df.corr(numeric_only=True)\n",
        "\n",
        "# sort by mort_acc correlation\n",
        "corr_with_mort_acc = corr_matrix['mort_acc'].sort_values(ascending=False)\n",
        "print(corr_with_mort_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2AHco0pENPr"
      },
      "source": [
        "The Most Correlated Column With `mort_acc` is `total_acc`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6A9H2LREqoo"
      },
      "outputs": [],
      "source": [
        "# Fill missing values in 'mort_acc' using median of corresponding 'total_acc' groups\n",
        "df['mort_acc'] = df['mort_acc'].fillna(\n",
        "    df.groupby('total_acc')['mort_acc'].transform('median')\n",
        ")\n",
        "print(\"missing Values in mort_acc = \",df['mort_acc'].isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-cD_JzKFTbw"
      },
      "source": [
        "#### Now Treat The Column `pub_rec_bankruptcies`\n",
        "- missing values : 535"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ds4qPlYoEjqy"
      },
      "outputs": [],
      "source": [
        "# the missing values are not that much\n",
        "df['pub_rec_bankruptcies'] = df['pub_rec_bankruptcies'].fillna(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpFrykxrJv9R"
      },
      "source": [
        "#### Column `loan_amnt`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18le0DEyKueO"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8,5))\n",
        "sns.histplot(df['loan_amnt'], bins=40, kde=True)\n",
        "plt.title(\"Distribution of Loan Amount\")\n",
        "plt.xlabel(\"Loan Amount\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nL8KdKooL_et"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8,5))\n",
        "sns.boxplot(x=df['loan_amnt'])\n",
        "plt.title(\"Boxplot of Loan Amount\")\n",
        "plt.xlabel(\"Loan Amount\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yba2ql7ZMk98"
      },
      "outputs": [],
      "source": [
        "Q1 = df['loan_amnt'].quantile(0.25)\n",
        "Q3 = df['loan_amnt'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "lower_limit = Q1 - 1.5*IQR\n",
        "upper_limit = Q3 + 1.5*IQR\n",
        "\n",
        "print(\"Lower Limit:\", lower_limit)\n",
        "print(\"Upper Limit:\", upper_limit)\n",
        "\n",
        "# Check outliers count\n",
        "outliers = df[(df['loan_amnt'] < lower_limit) | (df['loan_amnt'] > upper_limit)]\n",
        "print(\"Outliers found:\", outliers.shape[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Imia6p3NNKia"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(14,6))\n",
        "\n",
        "# Before\n",
        "plt.subplot(1,2,1)\n",
        "sns.boxplot(x=df['loan_amnt'], color=\"skyblue\")\n",
        "plt.title(\"Before Outlier Treatment (Loan Amount)\")\n",
        "#copy of df\n",
        "df_copy = df.copy()\n",
        "\n",
        "# Cap loan_amnt above 38000\n",
        "df_copy['loan_amnt_capped'] = np.where(df_copy['loan_amnt'] > 38000, 38000, df_copy['loan_amnt'])\n",
        "\n",
        "# After\n",
        "plt.subplot(1,2,2)\n",
        "sns.boxplot(x=df_copy['loan_amnt_capped'], color=\"lightgreen\")\n",
        "plt.title(\"After Outlier Treatment (Loan Amount Capped at 38k)\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeTMNKWEQeFl"
      },
      "source": [
        "#### Column `term`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rD9-OOVgQTY9"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(6,4))\n",
        "sns.countplot(x=df['term'], palette=\"viridis\")\n",
        "plt.title(\"Distribution of Loan Terms\")\n",
        "plt.xlabel(\"Loan Term\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9kHg0-mQ_Xx"
      },
      "outputs": [],
      "source": [
        "term_counts = df['term'].value_counts()\n",
        "\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.pie(term_counts, labels=term_counts.index, autopct='%1.1f%%', startangle=90, colors=[\"#66b3ff\",\"#99ff99\"])\n",
        "plt.title(\"Loan Term Distribution (%)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENN2-_73SgbD"
      },
      "outputs": [],
      "source": [
        "df['term'] = df['term'].astype(str).str.extract(r'(\\d+)').astype(float).astype('Int64')\n",
        "print(df['term'].unique())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIEKHaRhkdcf"
      },
      "source": [
        "#### For `int_rate`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHWKotZpkD7r"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,6))\n",
        "sns.histplot(df['int_rate'], bins=30, kde=True, color=\"skyblue\")\n",
        "plt.title(\"Distribution of Interest Rate\")\n",
        "plt.xlabel(\"Interest Rate (%)\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cst1sSRckj2G"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8,5))\n",
        "sns.boxplot(x=df['int_rate'])\n",
        "plt.title(\"Boxplot of Interest Rate\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOVXwZKslfcS"
      },
      "source": [
        "The Outliers are in the range of 0 - 40 which is realastic so we will keep the outliers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qppOCKWymBjq"
      },
      "source": [
        "#### For `installment`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4jtAjIklfO8"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8,6))\n",
        "sns.histplot(df['installment'], bins=50, kde=True, color=\"skyblue\")\n",
        "plt.title(\"Distribution of Installment\")\n",
        "plt.xlabel(\"Installment Amount\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4p8vYWommeDk"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8,6))\n",
        "sns.boxplot(x=df['installment'], color=\"lightgreen\")\n",
        "plt.title(\"Boxplot of Installment\")\n",
        "plt.xlabel(\"Installment Amount\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NVvMbxXnYll"
      },
      "outputs": [],
      "source": [
        "# Summary stats for installment\n",
        "summary_stats = df['installment'].describe()[['min','max','mean']]\n",
        "median_val = df['installment'].median()\n",
        "\n",
        "print(\"Summary Statistics for Installment:\")\n",
        "print(f\"Minimum: {summary_stats['min']}\")\n",
        "print(f\"Maximum: {summary_stats['max']}\")\n",
        "print(f\"Mean: {round(summary_stats['mean'],4)}\")\n",
        "print(f\"Median: {median_val}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VP9ZY8h7ntYR"
      },
      "source": [
        "#### For `grade`\n",
        "- Loan grade (A‚ÄìG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_aaZFlyYoPnz"
      },
      "outputs": [],
      "source": [
        "# Count of each grade\n",
        "grade_counts = df['grade'].value_counts(normalize=True) * 100   # % me convert\n",
        "grades = grade_counts.index\n",
        "percentages = grade_counts.values\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.barplot(x=grades, y=percentages, palette=\"viridis\")\n",
        "\n",
        "# Har bar ke upar % text\n",
        "for i, val in enumerate(percentages):\n",
        "    plt.text(i, val+0.5, f\"{val:.1f}%\", ha='center', fontsize=12, fontweight='bold')\n",
        "\n",
        "plt.title(\"Loan Grade Distribution (%)\", fontsize=14)\n",
        "plt.xlabel(\"Grade\")\n",
        "plt.ylabel(\"Percentage\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7EZmLRU574i"
      },
      "source": [
        "- B is the most common grade\n",
        "- G is the lowest common grade"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LwtvRc646iR"
      },
      "source": [
        "##### For `sub_grade`\n",
        "-**bold text** Loan subgrade (e.g., A1, A2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9d9Ciw4L5DYL"
      },
      "outputs": [],
      "source": [
        "# Create grade bin\n",
        "df['grade_bin'] = df['sub_grade'].str[0]\n",
        "\n",
        "# Loop through each grade bin and plot\n",
        "for grade in sorted(df['grade_bin'].unique()):\n",
        "    subgrade_subset = df[df['grade_bin'] == grade]['sub_grade']\n",
        "    counts = subgrade_subset.value_counts().sort_index()\n",
        "    percent = (counts / counts.sum()) * 100\n",
        "\n",
        "    # Display table\n",
        "    print(f\"\\nPercentage distribution for Grade {grade}:\\n\")\n",
        "    print(percent)\n",
        "\n",
        "    # Barplot\n",
        "    plt.figure(figsize=(8,4))\n",
        "    sns.barplot(x=percent.index, y=percent.values, palette=\"magma\")\n",
        "    plt.title(f\"Percentage Distribution - Grade {grade}\")\n",
        "    plt.ylabel(\"Percentage (%)\")\n",
        "    plt.xlabel(\"Sub Grade\")\n",
        "    for i, v in enumerate(percent.values):\n",
        "        plt.text(i, v + 0.5, f\"{v:.1f}%\", ha='center')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3RGA6jH72zz"
      },
      "outputs": [],
      "source": [
        "# Mode for grade\n",
        "grade_mode = df['grade'].mode()[0]\n",
        "print(f\"Most frequent Grade: {grade_mode}\")\n",
        "\n",
        "# Mode for sub_grade\n",
        "subgrade_mode = df['sub_grade'].mode()[0]\n",
        "print(f\"Most frequent Sub Grade: {subgrade_mode}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdoDd0eb74lu"
      },
      "source": [
        "#### For `home_ownership` :  Home ownership status"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IpwH-j3X8l3Q"
      },
      "outputs": [],
      "source": [
        "# Lowercase, strip spaces, remove non-alphanumeric (except underscore)\n",
        "df['home_ownership'] = df['home_ownership'].str.lower().str.strip().str.replace(r'[^a-z_]', '', regex=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvV_u5lC-L-3"
      },
      "outputs": [],
      "source": [
        "# Count per home_ownership category\n",
        "home_counts = df['home_ownership'].value_counts()\n",
        "print(home_counts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R8h_arPx-t9i"
      },
      "outputs": [],
      "source": [
        "home_counts = df['home_ownership'].value_counts()\n",
        "home_percent = (home_counts / home_counts.sum()) * 100\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.barplot(x=home_percent.index, y=home_percent.values, palette=\"viridis\")\n",
        "plt.ylabel(\"Percentage (%)\")\n",
        "plt.xlabel(\"Home Ownership\")\n",
        "plt.title(\"Home Ownership Percentage Distribution\")\n",
        "for i, v in enumerate(home_percent.values):\n",
        "    plt.text(i, v + 0.5, f\"{v:.4f}%\", ha='center')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ou6M7yw_CHH"
      },
      "source": [
        "**Summary Stats** :    \n",
        "- People with Mortgage : `50.0841 `%\n",
        "- People with Rent : `40.3480`%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYYALh62AxAT"
      },
      "outputs": [],
      "source": [
        "# Combine rare categories in home_ownership\n",
        "df['home_ownership'] = df['home_ownership'].replace(\n",
        "    ['none', 'any', 'other'], 'other'\n",
        ")\n",
        "\n",
        "# Check the updated distribution\n",
        "print(df['home_ownership'].value_counts(normalize=True) * 100)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSlACaPmA3Oc"
      },
      "source": [
        "#### For `annual_inc`\n",
        "-\tAnnual income (self-reported)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JpPBOCBtG2mh"
      },
      "outputs": [],
      "source": [
        "# Define income bins\n",
        "bins = [0, 25000, 50000, 75000, 100000, 150000, 200000, df['annual_inc'].max()]\n",
        "labels = ['0-25k','25k-50k','50k-75k','75k-100k','100k-150k','150k-200k','200k+']\n",
        "\n",
        "df['income_bin'] = pd.cut(df['annual_inc'], bins=bins, labels=labels, include_lowest=True)\n",
        "\n",
        "# Countplot\n",
        "plt.figure(figsize=(10,6))\n",
        "df['income_bin'].value_counts().sort_index().plot(kind='bar', color=\"skyblue\", edgecolor=\"black\")\n",
        "\n",
        "plt.title(\"Distribution of Annual Income Ranges\")\n",
        "plt.xlabel(\"Income Range\")\n",
        "plt.ylabel(\"Number of Borrowers\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2p0Iw5fdH9BB"
      },
      "source": [
        "##### Checking Outliers and Treating outlier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0g_wK_ji_aap"
      },
      "outputs": [],
      "source": [
        "Q1 = df['annual_inc'].quantile(0.25)\n",
        "Q3 = df['annual_inc'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "print(f\"Lower Bound: {lower_bound}, Upper Bound: {upper_bound}\")\n",
        "outliers = df[(df['annual_inc'] < lower_bound) | (df['annual_inc'] > upper_bound)]\n",
        "print(f\"Number of outliers: {len(outliers)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OoiKLhU9GkE-"
      },
      "outputs": [],
      "source": [
        "# Cap using IQR\n",
        "df['annual_inc_iqr'] = np.where(df['annual_inc'] > upper_bound, upper_bound,\n",
        "                                np.where(df['annual_inc'] < lower_bound, lower_bound, df['annual_inc']))\n",
        "\n",
        "# Boxplots before vs after\n",
        "plt.figure(figsize=(12,5))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "sns.boxplot(y=df['annual_inc'])\n",
        "plt.title(\"Annual Income - Raw (With Outliers)\")\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "sns.boxplot(y=df['annual_inc_iqr'])\n",
        "plt.title(\"Annual Income - After IQR Capping\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPAo26ltNo1O"
      },
      "source": [
        "#### For `verification_status`\n",
        "- Income verification status"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z12pK8VxO4JS"
      },
      "outputs": [],
      "source": [
        "# Percentage counts\n",
        "data = df['verification_status'].value_counts(normalize=True) * 100\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(5,8))\n",
        "plt.bar(data.index, data.values, color='lightblue', edgecolor='black')\n",
        "\n",
        "# Labels and title\n",
        "plt.xlabel(\"Verification Status\")\n",
        "plt.ylabel(\"Percentage (%)\")\n",
        "plt.title(\"Verification Status Distribution (in %)\")\n",
        "\n",
        "# Show values on top of bars\n",
        "for i, v in enumerate(data.values):\n",
        "    plt.text(i, v + 1, f\"{v:.1f}%\", ha='center')\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-Heb-JbTeyd"
      },
      "source": [
        "#### For `issue_d`\n",
        "- Loan issued date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ctK4REMWTaOv"
      },
      "outputs": [],
      "source": [
        "# convert issue_date into date time format\n",
        "df['issue_d'] = pd.to_datetime(df['issue_d'])\n",
        "df['issue_year'] = df['issue_d'].dt.year\n",
        "df['issue_year'].value_counts().sort_index().plot(kind='bar')\n",
        "plt.title('Loan For The Year')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tIfWRm2kUvrv"
      },
      "outputs": [],
      "source": [
        "df['issue_month'] = df['issue_d'].dt.month\n",
        "df['issue_month'].value_counts().sort_index().plot(kind='bar')\n",
        "plt.title('Month wise Loan')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Htdk9aIRUuHB"
      },
      "outputs": [],
      "source": [
        "#time series  line Graph\n",
        "df.groupby(df['issue_d'].dt.to_period('M')).size().plot(kind='line')\n",
        "plt.title('Time Series Line Graph')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knEflFAZVVfA"
      },
      "outputs": [],
      "source": [
        "# Seasonality check (Quarterly)\n",
        "df['issue_quarter'] = df['issue_d'].dt.quarter\n",
        "df['issue_quarter'].value_counts().sort_index().plot(kind='bar')\n",
        "plt.title('Season Wise Loan')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7ed69PKVujo"
      },
      "source": [
        "#### For `loan_status`  \n",
        "-\tTarget variable (Fully Paid / Charged Off)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMLHqdKKV5xy"
      },
      "outputs": [],
      "source": [
        "sns.countplot(y=df['loan_status'],)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4A0e-6rWQMq"
      },
      "source": [
        "`Note` : The Number of Fully Paid **Is Greater Than** Charged oFF people\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOwtxYLOWlnh"
      },
      "source": [
        "#### For `purpose`\n",
        "- Loan Purpose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rv9Avp4jYMkr"
      },
      "outputs": [],
      "source": [
        "purpose_counts = df['purpose'].value_counts().reset_index()\n",
        "purpose_counts.columns = ['purpose', 'count']\n",
        "purpose_counts['percentage'] = (purpose_counts['count'] / purpose_counts['count'].sum()) * 100\n",
        "\n",
        "purpose_counts['cumulative_percent'] = purpose_counts['percentage'].cumsum()\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.bar(purpose_counts['purpose'], purpose_counts['count'], color='skyblue', edgecolor='black')\n",
        "\n",
        "for i, v in enumerate(purpose_counts['count']):\n",
        "    plt.text(i, v+1000, f\"{purpose_counts['percentage'].iloc[i]:.1f}%\", ha='center')\n",
        "\n",
        "plt.xticks(rotation=45)\n",
        "plt.xlabel(\"Loan Purpose\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.title(\"Loan Purpose Distribution with %\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VfNzXlmIWkKP"
      },
      "outputs": [],
      "source": [
        "df['purpose'] = df['purpose'].str.lower().str.strip().str.replace(r'[^a-z\\s]', '', regex=True)\n",
        "df['purpose'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aR0egy3ZA_3"
      },
      "source": [
        "##### Loan Purpose Category Merging\n",
        "\n",
        "### üîπ Original Categories ‚Üí Grouped Categories\n",
        "\n",
        "- **Debt Related**\n",
        "  - `debtconsolidation` ‚Üí **debt**\n",
        "  - `creditcard` ‚Üí **debt**\n",
        "\n",
        "- **Housing / Asset Related**\n",
        "  - `homeimprovement` ‚Üí **housing**\n",
        "  - `house` ‚Üí **housing**\n",
        "  - `car` ‚Üí **housing**\n",
        "\n",
        "- **Business Related**\n",
        "  - `smallbusiness` ‚Üí **business**\n",
        "\n",
        "- **Purchase Related**\n",
        "  - `majorpurchase` ‚Üí **purchase**\n",
        "\n",
        "- **Personal Expenses**\n",
        "  - `medical` ‚Üí **personal**\n",
        "  - `vacation` ‚Üí **personal**\n",
        "  - `wedding` ‚Üí **personal**\n",
        "  - `moving` ‚Üí **personal**\n",
        "  - `other` ‚Üí **personal**\n",
        "\n",
        "- **Education**\n",
        "  - `educational` ‚Üí **education**\n",
        "\n",
        "- **Others**\n",
        "  - `renewableenergy` ‚Üí **others**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGG9-S4oZtG7"
      },
      "outputs": [],
      "source": [
        "# Mapping for merging categories\n",
        "mapping = {\n",
        "    'debtconsolidation': 'debt',\n",
        "    'creditcard': 'debt',\n",
        "    'homeimprovement': 'housing',\n",
        "    'house': 'housing',\n",
        "    'car': 'housing',\n",
        "    'smallbusiness': 'business',\n",
        "    'majorpurchase': 'purchase',\n",
        "    'medical': 'personal',\n",
        "    'vacation': 'personal',\n",
        "    'wedding': 'personal',\n",
        "    'moving': 'personal',\n",
        "    'other': 'personal',\n",
        "    'educational': 'education',\n",
        "    'renewableenergy': 'others'\n",
        "}\n",
        "\n",
        "# Apply mapping\n",
        "df['purpose_grouped'] = df['purpose'].map(mapping)\n",
        "\n",
        "# Original counts\n",
        "orig_counts = df['purpose'].value_counts().reset_index()\n",
        "orig_counts.columns = ['purpose', 'count']\n",
        "\n",
        "# Grouped counts\n",
        "grouped_counts = df['purpose_grouped'].value_counts().reset_index()\n",
        "grouped_counts.columns = ['purpose_grouped', 'count']\n",
        "\n",
        "# Plot side by side\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16,6))\n",
        "\n",
        "# Before (original)\n",
        "axes[0].bar(orig_counts['purpose'], orig_counts['count'], color='lightcoral', edgecolor='black')\n",
        "axes[0].set_title(\"Before Grouping (Original Purposes)\")\n",
        "axes[0].set_xlabel(\"Loan Purpose\")\n",
        "axes[0].set_ylabel(\"Count\")\n",
        "axes[0].tick_params(axis='x', rotation=90)\n",
        "\n",
        "# After (grouped)\n",
        "axes[1].bar(grouped_counts['purpose_grouped'], grouped_counts['count'], color='lightblue', edgecolor='black')\n",
        "axes[1].set_title(\"After Grouping (Merged Purposes)\")\n",
        "axes[1].set_xlabel(\"Loan Purpose (Grouped)\")\n",
        "axes[1].set_ylabel(\"Count\")\n",
        "axes[1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNYq9hNfY6DV"
      },
      "outputs": [],
      "source": [
        "# Define mapping\n",
        "mapping = {\n",
        "    'debtconsolidation': 'debt',\n",
        "    'creditcard': 'debt',\n",
        "    'homeimprovement': 'housing',\n",
        "    'house': 'housing',\n",
        "    'car': 'housing',\n",
        "    'smallbusiness': 'business',\n",
        "    'majorpurchase': 'purchase',\n",
        "    'medical': 'personal',\n",
        "    'vacation': 'personal',\n",
        "    'wedding': 'personal',\n",
        "    'moving': 'personal',\n",
        "    'other': 'personal',\n",
        "    'educational': 'education',\n",
        "    'renewableenergy': 'others'\n",
        "}\n",
        "\n",
        "# Apply mapping\n",
        "df['purpose_grouped'] = df['purpose'].map(mapping)\n",
        "df['purpose_grouped']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64IDD_uGaREp"
      },
      "outputs": [],
      "source": [
        "df['purpose_grouped'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KviP_jv-bzmO"
      },
      "source": [
        "#### For `dti`\n",
        "- Debt-to-Income ratio\n",
        "- Lower DTI (<35%) ‚Üí Safe borrower\n",
        "- Higher DTI (>40‚Äì45%) ‚Üí Risky borrower\n",
        "\n",
        "\n",
        "- **Note** : Ye batata hai ki borrower apne income ka kitna % part loan aur debt chukane me kharch kar raha hai."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzlOn839bc-I"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,6))\n",
        "sns.histplot(df['dti'], bins=50, kde=True, color=\"purple\")\n",
        "plt.title(\"Distribution of DTI\")\n",
        "plt.xlabel(\"Debt-to-Income Ratio\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1oQOUPd_dNlK"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def detect_and_cap_outliers(df, column, plot=True):\n",
        "    Q1 = df[column].quantile(0.25)\n",
        "    Q3 = df[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "\n",
        "    lower_limit = Q1 - 1.5 * IQR\n",
        "    upper_limit = Q3 + 1.5 * IQR\n",
        "\n",
        "    outliers = df[(df[column] < lower_limit) | (df[column] > upper_limit)]\n",
        "\n",
        "    print(f\"üìå Column: {column}\")\n",
        "    print(f\"Q1: {Q1}, Q3: {Q3}, IQR: {round(IQR,4)}\")\n",
        "    print(f\"Lower Limit: {round(lower_limit,4)}, Upper Limit: {round(upper_limit,4)}\")\n",
        "    print(f\"Outliers Found: {outliers.shape[0]}\")\n",
        "\n",
        "    # Cap values\n",
        "    df[column + '_capped'] = df[column].clip(lower=lower_limit, upper=upper_limit)\n",
        "\n",
        "    if plot:\n",
        "        plt.figure(figsize=(12,6))\n",
        "\n",
        "        plt.subplot(1,2,1)\n",
        "        sns.boxplot(y=df[column], color=\"skyblue\")\n",
        "        plt.title(f\"Before Outlier Treatment ({column})\")\n",
        "\n",
        "        plt.subplot(1,2,2)\n",
        "        sns.boxplot(y=df[column + '_capped'], color=\"lightgreen\")\n",
        "        plt.title(f\"After Outlier Treatment ({column})\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    return df, lower_limit, upper_limit, outliers\n",
        "\n",
        "# Run for dti\n",
        "df, lower_limit, upper_limit, outliers = detect_and_cap_outliers(df, 'dti')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbLseR9ugMuC"
      },
      "source": [
        "#### For `earliest_cr_line`\n",
        "- \tDate of earliest credit line"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZRNm2PIgMJQ"
      },
      "outputs": [],
      "source": [
        "df['earliest_cr_line'] = pd.to_datetime(df['earliest_cr_line'], errors='coerce')\n",
        "df['credit_year'] = df['earliest_cr_line'].dt.year\n",
        "df['credit_history_length'] = (pd.to_datetime('today') - df['earliest_cr_line']).dt.days // 365"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGzRoelfgsvO"
      },
      "outputs": [],
      "source": [
        "# Histogram of year\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.histplot(df['credit_year'], bins=30, kde=True)\n",
        "plt.title(\"Distribution of Earliest Credit Line Year\")\n",
        "plt.xlabel(\"Year\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()\n",
        "\n",
        "# Histogram of credit history length\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.histplot(df['credit_history_length'], bins=30, kde=True, color=\"green\")\n",
        "plt.title(\"Distribution of Credit History Length (Years)\")\n",
        "plt.xlabel(\"Years\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pw1EuTxQhTjm"
      },
      "outputs": [],
      "source": [
        "def cap_outliers_iqr(df, column):\n",
        "    Q1 = df[column].quantile(0.25)\n",
        "    Q3 = df[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "\n",
        "    lower_limit = Q1 - 1.5 * IQR\n",
        "    upper_limit = Q3 + 1.5 * IQR\n",
        "\n",
        "    print(f\"üìå Column: {column}\")\n",
        "    print(f\"Q1: {Q1}, Q3: {Q3}, IQR: {round(IQR,4)}\")\n",
        "    print(f\"Lower Limit: {round(lower_limit,4)}, Upper Limit: {round(upper_limit,4)}\")\n",
        "\n",
        "    # capped column\n",
        "    df[column + \"_capped\"] = df[column].clip(lower=lower_limit, upper=upper_limit)\n",
        "\n",
        "    # before vs after plot\n",
        "    fig, axes = plt.subplots(1,2, figsize=(12,5))\n",
        "\n",
        "    sns.boxplot(y=df[column], ax=axes[0], color=\"skyblue\")\n",
        "    axes[0].set_title(f\"Before Outlier Treatment ({column})\")\n",
        "\n",
        "    sns.boxplot(y=df[column + \"_capped\"], ax=axes[1], color=\"lightgreen\")\n",
        "    axes[1].set_title(f\"After Outlier Treatment ({column})\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return df\n",
        "\n",
        "# Example run\n",
        "df = cap_outliers_iqr(df, 'credit_history_length')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcbWrhE_iyjM"
      },
      "source": [
        "#### For `open_acc`\n",
        "- \tNumber of open credit lines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kIW5jH4siwzg"
      },
      "outputs": [],
      "source": [
        "df=detect_and_cap_outliers(df,'open_acc')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fSiAkrx-gWpm"
      },
      "outputs": [],
      "source": [
        "df=df[0]\n",
        "df=pd.DataFrame(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNz8mwgjkPOV"
      },
      "source": [
        "#### For `Address`\n",
        "- address is not that nessary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyUd3JsnrbUR"
      },
      "outputs": [],
      "source": [
        "df = df.drop(columns=['address'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EkHoMtj2_fS5"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7EIxcxRDPr7"
      },
      "source": [
        "# Bivariate Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4MAgkCCDowX"
      },
      "source": [
        "#### `emp_length` is converted into `emp_experience_level`  \n",
        "- `emp_experience_level` vs `loan_status`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMKU7P6dp-t9"
      },
      "outputs": [],
      "source": [
        "def categorize_experience(x):\n",
        "    if x in [0.0, 1.0]:\n",
        "        return \"Entry-level\"\n",
        "    elif x in [2.0, 3.0, 4.0]:\n",
        "        return \"Junior\"\n",
        "    elif x in [5.0, 6.0, 7.0, 8.0, 9.0]:\n",
        "        return \"Mid-level\"\n",
        "    elif x == 10.0:\n",
        "        return \"Experienced\"\n",
        "    else:\n",
        "        return \"Unknown\"\n",
        "\n",
        "df['emp_experience_level'] = df['emp_length'].apply(categorize_experience)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lf9JBaZpBF1u"
      },
      "outputs": [],
      "source": [
        "# Group + reset\n",
        "data = df.groupby('emp_experience_level')['loan_status'].value_counts().reset_index(name='count')\n",
        "\n",
        "# Barplot\n",
        "plt.figure(figsize=(10,6))\n",
        "ax = sns.barplot(x='emp_experience_level', y='count', hue='loan_status', data=data)\n",
        "\n",
        "# Add labels on top of bars\n",
        "from matplotlib import ticker\n",
        "for container in ax.containers:\n",
        "    ax.bar_label(container, fmt='%d', label_type='edge', padding=3, fontsize=9)\n",
        "\n",
        "plt.title(\"Loan Status by Experience Level\")\n",
        "plt.xlabel(\"Experience Level\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgMAsz3EENYy"
      },
      "source": [
        "#### `loan_amnt` vs `annual_income`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "si90OI0XELHn"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20,20))\n",
        "sns.heatmap(df.corr(numeric_only=True), annot=True, cmap=\"coolwarm\")\n",
        "plt.title(\"Correlation Heatmap\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kqa4fNqITlj"
      },
      "source": [
        "For `int_rate` vs `loan_status`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nvQh-g6lGnyr"
      },
      "outputs": [],
      "source": [
        "# Step 1: Make 5% bins\n",
        "max_rate = df['int_rate'].max()\n",
        "bins = np.arange(0, max_rate+5, 5)  # 0, 5, 10, 15, ...\n",
        "labels = [f\"{int(b)}-{int(b+5)}%\" for b in bins[:-1]]\n",
        "df['int_rate_bin'] = pd.cut(df['int_rate'], bins=bins, labels=labels, right=False)\n",
        "\n",
        "# Step 2: Group + percentage\n",
        "data = (\n",
        "    df.groupby('int_rate_bin')['loan_status']\n",
        "    .value_counts(normalize=True)\n",
        "    .mul(100)\n",
        "    .reset_index(name='percentage')\n",
        ")\n",
        "\n",
        "# Step 3: Barplot\n",
        "plt.figure(figsize=(12,6))\n",
        "ax = sns.barplot(x='int_rate_bin', y='percentage', hue='loan_status', data=data)\n",
        "\n",
        "# Add % labels on top\n",
        "for p in ax.patches:\n",
        "    ax.text(\n",
        "        p.get_x() + p.get_width()/2.,\n",
        "        p.get_height() + 0.5,\n",
        "        f\"{p.get_height():.1f}%\",\n",
        "        ha='center', va='bottom', fontsize=9\n",
        "    )\n",
        "\n",
        "plt.title(\"Loan Status % by Interest Rate (5% bins)\")\n",
        "plt.xlabel(\"Interest Rate Range\")\n",
        "plt.ylabel(\"Percentage (%)\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFkMyCYUKj4N"
      },
      "source": [
        "For `annual_inc` Vs `loan_status`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddsat_uSKjO3"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Create income bins\n",
        "bins = [0, 25000, 50000, 75000, 100000, 150000, 200000, 500000, df['annual_inc'].max()]\n",
        "labels = ['0-25k', '25-50k', '50-75k', '75-100k', '100-150k', '150-200k', '200-500k', '500k+']\n",
        "df['income_bin'] = pd.cut(df['annual_inc'], bins=bins, labels=labels, include_lowest=True)\n",
        "\n",
        "# Calculate percentage of loan_status within each income_bin\n",
        "status_counts = df.groupby(['income_bin','loan_status']).size().unstack(fill_value=0)\n",
        "status_pct = status_counts.div(status_counts.sum(axis=1), axis=0) * 100\n",
        "\n",
        "# Plot\n",
        "ax = status_pct.plot(kind='bar', stacked=False, figsize=(12,6))\n",
        "plt.title(\"Loan Status % by Annual Income Range\")\n",
        "plt.ylabel(\"Percentage (%)\")\n",
        "plt.xlabel(\"Annual Income Range\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend(title=\"loan_status\")\n",
        "\n",
        "# Add percentage labels on bars\n",
        "for p in ax.patches:\n",
        "    width = p.get_width()\n",
        "    height = p.get_height()\n",
        "    x, y = p.get_xy()\n",
        "    if height > 0:  # show only if >0\n",
        "        ax.text(x + width/2,\n",
        "                y + height/2,\n",
        "                f'{height:.1f}%',\n",
        "                ha='center', va='bottom', fontsize=8, color='black')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szmMHIG9Mnhk"
      },
      "source": [
        "For `Expericed` Vs `Annual income`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QFcaGBuVMmw7"
      },
      "outputs": [],
      "source": [
        "# Step 1: Bin annual_inc\n",
        "bins = [0, 25000, 50000, 75000, 100000, 150000, 200000, 500000, df['annual_inc'].max()]\n",
        "labels = ['0-25k', '25-50k', '50-75k', '75-100k', '100-150k', '150-200k', '200-500k', '500k+']\n",
        "df['income_bin'] = pd.cut(df['annual_inc'], bins=bins, labels=labels, include_lowest=True)\n",
        "\n",
        "# Step 2: Group and count\n",
        "counts = df.groupby(['income_bin','emp_experience_level']).size().reset_index(name='count')\n",
        "\n",
        "# Step 3: Plot\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.barplot(data=counts, x='income_bin', y='count', hue='emp_experience_level')\n",
        "plt.title(\"Employee Experience Level by Annual Income Range\")\n",
        "plt.xlabel(\"Annual Income Range\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend(title=\"Experience Level\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzMlz1wcQL8x"
      },
      "source": [
        "# Machine Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kXKL-ApQQTP"
      },
      "source": [
        "#### Creating New Df For Machine Learnig\n",
        "- The Df is Cleaned version of the original Df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yjlYFNAUPrtm"
      },
      "outputs": [],
      "source": [
        "# Features for ML\n",
        "ml_features = [\n",
        "    'loan_amnt', 'term', 'int_rate', 'installment',\n",
        "    'emp_length', 'annual_inc', 'dti_capped',\n",
        "    'open_acc', 'total_acc', 'pub_rec', 'pub_rec_bankruptcies',\n",
        "    'revol_bal', 'revol_util', 'mort_acc', 'credit_history_length_capped',\n",
        "    'grade', 'sub_grade', 'home_ownership', 'verification_status',\n",
        "    'application_type', 'initial_list_status', 'purpose_grouped',\n",
        "    'emp_experience_level', 'issue_year', 'issue_month', 'issue_quarter'\n",
        "]\n",
        "\n",
        "# Create new dataframe\n",
        "df_ml = df[ml_features + ['loan_status']]  # loan_status = target\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ya9c0LODRD7t"
      },
      "source": [
        "‚úÖ Step 1: Split features & target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICHCgErnQnXy"
      },
      "outputs": [],
      "source": [
        "X = df_ml.drop('loan_status', axis=1)\n",
        "y = df_ml['loan_status'].apply(lambda x: 1 if x=='Charged Off' else 0)   # 1 = Default, 0 = Fully Paid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qgJz7n7RFwi"
      },
      "source": [
        "‚úÖ Step 2: Train-test split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWO6y_H6Q0Y3"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgTLAtSBRICY"
      },
      "source": [
        "‚úÖ Step 3: Preprocessing (encode + scale)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJS3bxotQ3-L"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "numeric_features = ['loan_amnt','term','int_rate','installment',\n",
        "                    'emp_length','annual_inc','dti_capped',\n",
        "                    'open_acc','total_acc','pub_rec','pub_rec_bankruptcies',\n",
        "                    'revol_bal','revol_util','mort_acc','credit_history_length_capped',\n",
        "                    'issue_year','issue_month','issue_quarter']\n",
        "\n",
        "categorical_features = ['grade','sub_grade','home_ownership',\n",
        "                        'verification_status','application_type',\n",
        "                        'initial_list_status','purpose_grouped',\n",
        "                        'emp_experience_level']\n",
        "\n",
        "numeric_transformer = StandardScaler()\n",
        "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4snzL1uLRKUr"
      },
      "source": [
        "‚úÖ Step 4: Logistic Regression model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ai2PevHpQ7V4"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression(max_iter=1000, solver='lbfgs'))\n",
        "])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HC9tNWn_RNCo"
      },
      "source": [
        "‚úÖ Step 5: Train & Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWCh2YaSQ-pe"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S397wvqORfbe"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ---------------------------\n",
        "# Step 1: Features & Target\n",
        "# ---------------------------\n",
        "X = df_ml.drop('loan_status', axis=1)\n",
        "y = df_ml['loan_status'].apply(lambda x: 1 if x=='Charged Off' else 0)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# ---------------------------\n",
        "# Step 2: Preprocessor\n",
        "# ---------------------------\n",
        "numeric_features = ['loan_amnt','term','int_rate','installment',\n",
        "                    'emp_length','annual_inc','dti_capped',\n",
        "                    'open_acc','total_acc','pub_rec','pub_rec_bankruptcies',\n",
        "                    'revol_bal','revol_util','mort_acc','credit_history_length_capped',\n",
        "                    'issue_year','issue_month','issue_quarter']\n",
        "\n",
        "categorical_features = ['grade','sub_grade','home_ownership',\n",
        "                        'verification_status','application_type',\n",
        "                        'initial_list_status','purpose_grouped',\n",
        "                        'emp_experience_level']\n",
        "\n",
        "numeric_transformer = StandardScaler()\n",
        "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n",
        "\n",
        "# ---------------------------\n",
        "# Step 3: Apply SMOTE\n",
        "# ---------------------------\n",
        "smote = SMOTE(random_state=42)\n",
        "\n",
        "# First preprocess features\n",
        "X_train_transformed = preprocessor.fit_transform(X_train)\n",
        "X_test_transformed = preprocessor.transform(X_test)\n",
        "\n",
        "# Apply SMOTE on transformed features\n",
        "X_res, y_res = smote.fit_resample(X_train_transformed, y_train)\n",
        "\n",
        "# ---------------------------\n",
        "# Step 4: Logistic Regression\n",
        "# ---------------------------\n",
        "clf = LogisticRegression(max_iter=1000, solver='lbfgs')\n",
        "clf.fit(X_res, y_res)\n",
        "\n",
        "# ---------------------------\n",
        "# Step 5: Evaluation\n",
        "# ---------------------------\n",
        "y_pred = clf.predict(X_test_transformed)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# ROC-AUC Curve\n",
        "y_pred_prob = clf.predict_proba(X_test_transformed)[:,1]\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
        "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
        "\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.plot(fpr, tpr, label=f\"ROC curve (AUC = {roc_auc:.2f})\")\n",
        "plt.plot([0,1],[0,1],'k--')\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC-AUC Curve (Logistic Regression + SMOTE)\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cq4ENUJfbDzQ"
      },
      "source": [
        "#### For Randomn Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HquElvNCVIbv"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Make a copy\n",
        "X_train_enc = X_train.copy()\n",
        "X_test_enc = X_test.copy()\n",
        "\n",
        "# Encode categorical features\n",
        "cat_cols = ['grade', 'sub_grade', 'home_ownership', 'verification_status',\n",
        "            'application_type', 'initial_list_status', 'purpose_grouped',\n",
        "            'emp_experience_level']\n",
        "\n",
        "le = LabelEncoder()\n",
        "\n",
        "for col in cat_cols:\n",
        "    X_train_enc[col] = le.fit_transform(X_train_enc[col])\n",
        "    X_test_enc[col] = le.transform(X_test_enc[col])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ejJtJPwjbDgA",
        "outputId": "642585cb-9da8-4676-a040-7490575a51ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.689846728783173\n",
            "\n",
            "Confusion Matrix:\n",
            " [[44981 18690]\n",
            " [ 5876  9659]]\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.71      0.79     63671\n",
            "           1       0.34      0.62      0.44     15535\n",
            "\n",
            "    accuracy                           0.69     79206\n",
            "   macro avg       0.61      0.66      0.61     79206\n",
            "weighted avg       0.78      0.69      0.72     79206\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# ---------------------------\n",
        "# Step 1: Ensure y is integer\n",
        "# ---------------------------\n",
        "y_train_int = y_train.astype(int)\n",
        "y_test_int = y_test.astype(int)\n",
        "\n",
        "X_train_np = X_train_enc.values\n",
        "X_test_np = X_test_enc.values\n",
        "\n",
        "# ---------------------------\n",
        "# Step 2: Random Forest Classifier\n",
        "# ---------------------------\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=300,   # number of trees\n",
        "    max_depth=12,      # depth of each tree\n",
        "    random_state=42,\n",
        "    class_weight=\"balanced\"   # handle imbalance\n",
        ")\n",
        "\n",
        "rf.fit(X_train_np, y_train_int)\n",
        "y_pred = rf.predict(X_test_np)\n",
        "\n",
        "# ---------------------------\n",
        "# Step 3: Evaluation\n",
        "# ---------------------------\n",
        "print(\"Accuracy:\", accuracy_score(y_test_int, y_pred))\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test_int, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test_int, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzGwx03lUPaa"
      },
      "source": [
        "#### Using XG Boost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nFPfArKuUZBZ",
        "outputId": "ca17d060-2dc8-4b61-9f6a-46185f2b1e55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8092694997853698\n",
            "\n",
            "Confusion Matrix:\n",
            " [[62390  1281]\n",
            " [13826  1709]]\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.98      0.89     63671\n",
            "           1       0.57      0.11      0.18     15535\n",
            "\n",
            "    accuracy                           0.81     79206\n",
            "   macro avg       0.70      0.54      0.54     79206\n",
            "weighted avg       0.77      0.81      0.75     79206\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# ---------------------------\n",
        "# Step 1: Ensure y is integer\n",
        "# ---------------------------\n",
        "y_train_int = y_train.astype(int)\n",
        "y_test_int = y_test.astype(int)\n",
        "\n",
        "X_train_np = X_train_enc.values\n",
        "X_test_np = X_test_enc.values\n",
        "\n",
        "# ---------------------------\n",
        "# Step 2: XGBoost Classifier\n",
        "# ---------------------------\n",
        "xgb = XGBClassifier(\n",
        "    n_estimators=500,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=6,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    reg_lambda=10,        # L2 regularization\n",
        "    random_state=42,\n",
        "    eval_metric=\"logloss\"\n",
        ")\n",
        "\n",
        "xgb.fit(X_train_np, y_train_int)\n",
        "y_pred = xgb.predict(X_test_np)\n",
        "\n",
        "# ---------------------------\n",
        "# Step 3: Evaluation\n",
        "# ---------------------------\n",
        "print(\"Accuracy:\", accuracy_score(y_test_int, y_pred))\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test_int, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test_int, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "scqkjkUf9al7",
        "outputId": "b58221f6-a713-4950-bfc3-c9ae0478859e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Threshold | Precision(1) | Recall(1) | F1(1)\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'y_pred_proba' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3076665462.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthresholds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_pred_proba\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     precision, recall, f1, _ = precision_recall_fscore_support(\n\u001b[1;32m     11\u001b[0m         \u001b[0my_test_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'y_pred_proba' is not defined"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "thresholds = np.arange(0.1, 0.9, 0.1)\n",
        "\n",
        "print(\"Threshold | Precision(1) | Recall(1) | F1(1)\")\n",
        "print(\"-\"*50)\n",
        "\n",
        "for t in thresholds:\n",
        "    y_pred = (y_pred_proba >= t).astype(int)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        y_test_int, y_pred, average=None, labels=[1]\n",
        "    )\n",
        "    print(f\"{t:8.2f} | {precision[0]:11.3f} | {recall[0]:8.3f} | {f1[0]:6.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "879_rXAm5ZcG"
      },
      "outputs": [],
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "# ---------------------------\n",
        "# Step 1: Ensure y is integer\n",
        "# ---------------------------\n",
        "y_train_int = y_train.astype(int)\n",
        "y_test_int = y_test.astype(int)\n",
        "\n",
        "X_train_np = X_train_enc.values\n",
        "X_test_np = X_test_enc.values\n",
        "\n",
        "# ---------------------------\n",
        "# Step 2: Compute scale_pos_weight\n",
        "# ---------------------------\n",
        "neg, pos = np.bincount(y_train_int)   # count class distribution\n",
        "scale_pos_weight = neg / pos\n",
        "print(f\"scale_pos_weight = {scale_pos_weight:.2f}\")\n",
        "\n",
        "# ---------------------------\n",
        "# Step 3: XGBoost Classifier with imbalance handling\n",
        "# ---------------------------\n",
        "xgb = XGBClassifier(\n",
        "    n_estimators=500,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=6,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    reg_lambda=10,        # L2 regularization\n",
        "    random_state=42,\n",
        "    eval_metric=\"aucpr\",  # better for imbalance\n",
        "    scale_pos_weight=scale_pos_weight\n",
        ")\n",
        "\n",
        "xgb.fit(X_train_np, y_train_int)\n",
        "\n",
        "# ---------------------------\n",
        "# Step 4: Predict with threshold tuning\n",
        "# ---------------------------\n",
        "y_pred_proba = xgb.predict_proba(X_test_np)[:,1]\n",
        "\n",
        "# try lower threshold instead of 0.5 (e.g. 0.3)\n",
        "threshold = 0.3\n",
        "y_pred = (y_pred_proba >= threshold).astype(int)\n",
        "\n",
        "# ---------------------------\n",
        "# Step 5: Evaluation\n",
        "# ---------------------------\n",
        "print(\"Accuracy:\", accuracy_score(y_test_int, y_pred))\n",
        "print(\"ROC-AUC:\", roc_auc_score(y_test_int, y_pred_proba))\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test_int, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test_int, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9KYIBAt0d-Do"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ---------------------------\n",
        "# Step 1: Probabilities from all models\n",
        "# ---------------------------\n",
        "y_prob_logreg = clf.predict_proba(X_test_transformed)[:,1]   # Logistic Regression\n",
        "y_prob_rf = rf.predict_proba(X_test_np)[:,1]                 # Random Forest\n",
        "y_prob_xgb = xgb.predict_proba(X_test_enc.values)[:,1]       # XGBoost\n",
        "\n",
        "# ---------------------------\n",
        "# Step 2: ROC curve + AUC for each model\n",
        "# ---------------------------\n",
        "fpr_log, tpr_log, _ = roc_curve(y_test, y_prob_logreg)\n",
        "fpr_rf, tpr_rf, _   = roc_curve(y_test_int, y_prob_rf)\n",
        "fpr_xgb, tpr_xgb, _ = roc_curve(y_test, y_prob_xgb)\n",
        "\n",
        "auc_log = roc_auc_score(y_test, y_prob_logreg)\n",
        "auc_rf  = roc_auc_score(y_test_int, y_prob_rf)\n",
        "auc_xgb = roc_auc_score(y_test, y_prob_xgb)\n",
        "\n",
        "# ---------------------------\n",
        "# Step 3: Plot ROC curves\n",
        "# ---------------------------\n",
        "plt.figure(figsize=(7,7))\n",
        "plt.plot(fpr_log, tpr_log, label=f\"Logistic Regression (AUC = {auc_log:.2f})\")\n",
        "plt.plot(fpr_rf, tpr_rf, label=f\"Random Forest (AUC = {auc_rf:.2f})\")\n",
        "plt.plot(fpr_xgb, tpr_xgb, label=f\"XGBoost (AUC = {auc_xgb:.2f})\")\n",
        "\n",
        "plt.plot([0,1], [0,1], 'k--')  # baseline\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve Comparison: LogReg vs RF vs XGB\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TrMgnMJB_eoF"
      },
      "outputs": [],
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, confusion_matrix, classification_report, roc_auc_score,\n",
        "    precision_recall_curve, roc_curve, auc\n",
        ")\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ---------------------------\n",
        "# Step 1: Ensure y is integer\n",
        "# ---------------------------\n",
        "y_train_int = y_train.astype(int)\n",
        "y_test_int = y_test.astype(int)\n",
        "\n",
        "X_train_np = X_train_enc.values\n",
        "X_test_np = X_test_enc.values\n",
        "\n",
        "# ---------------------------\n",
        "# Step 2: Compute scale_pos_weight\n",
        "# ---------------------------\n",
        "neg, pos = np.bincount(y_train_int)   # count class distribution\n",
        "scale_pos_weight = neg / pos\n",
        "print(f\"scale_pos_weight = {scale_pos_weight:.2f}\")\n",
        "\n",
        "# ---------------------------\n",
        "# Step 3: XGBoost Classifier with imbalance handling\n",
        "# ---------------------------\n",
        "xgb = XGBClassifier(\n",
        "    n_estimators=500,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=6,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    reg_lambda=10,\n",
        "    random_state=42,\n",
        "    eval_metric=\"aucpr\",\n",
        "    scale_pos_weight=scale_pos_weight\n",
        ")\n",
        "\n",
        "xgb.fit(X_train_np, y_train_int)\n",
        "\n",
        "# ---------------------------\n",
        "# Step 4: Predict probabilities\n",
        "# ---------------------------\n",
        "y_pred_proba = xgb.predict_proba(X_test_np)[:, 1]\n",
        "\n",
        "# ---------------------------\n",
        "# Step 5: Precision-Recall Curve\n",
        "# ---------------------------\n",
        "precisions, recalls, thresholds_pr = precision_recall_curve(y_test_int, y_pred_proba)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(recalls, precisions, marker='.')\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(\"Precision-Recall Curve\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# ---------------------------\n",
        "# Step 6: ROC Curve\n",
        "# ---------------------------\n",
        "fpr, tpr, thresholds_roc = roc_curve(y_test_int, y_pred_proba)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(fpr, tpr, label=f\"ROC Curve (AUC = {roc_auc:.2f})\")\n",
        "plt.plot([0,1],[0,1],'--',color='gray')\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate (Recall)\")\n",
        "plt.title(\"ROC Curve\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# ---------------------------\n",
        "# Step 7: Final Evaluation (threshold=0.4 as example)\n",
        "# ---------------------------\n",
        "threshold = 0.5\n",
        "y_pred = (y_pred_proba >= threshold).astype(int)\n",
        "\n",
        "print(\"\\nFinal Model (Threshold=0.4)\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test_int, y_pred))\n",
        "print(\"ROC-AUC:\", roc_auc_score(y_test_int, y_pred_proba))\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test_int, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test_int, y_pred))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "IxH08-nj88Sc",
        "ia6vi0J19aW_",
        "dvQsIJ7G8y2k",
        "vZeda_w_BPeY",
        "TCfv1yhfB1WB",
        "d-cD_JzKFTbw",
        "aeTMNKWEQeFl",
        "TIEKHaRhkdcf",
        "qppOCKWymBjq",
        "VP9ZY8h7ntYR",
        "9LwtvRc646iR"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}